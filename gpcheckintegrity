#!/usr/bin/env python

import os
import string
import sys
import optparse
from threading import Thread, Lock, BoundedSemaphore

#TODO: Import these inside functions so that the parser help can be used at any given time
try:
    from gppylib.gparray import GpArray
    from gppylib.gphostcache import *
    from gppylib.gplog import *
    from gppylib.commands.unix import *
    from gppylib.commands.gp import *
    from gppylib.db import dbconn
    from gppylib.userinput import *
    from pygresql.pg import DatabaseError
    from pygresql.pgdb import DatabaseError
    from pygresql import pg
    from datetime import datetime

except ImportError, e:
    sys.exit(str(e)+ '\nHint: Make sure that greenplum_path.sh is sourced')

##############
EXECNAME = os.path.split(__file__)[-1]
VERSION = '1.0'
DEFAULT_NUM_THREADS = 32

##############


def parseargs():
    parser = optparse.OptionParser(prog='gpcheckintegrity', usage="%prog [options] -d database | -A ", version=VERSION,
                                   description='Greenplum utility that performs an integrity check to verify'
                                               ' that the information stored in tables can be accessed.',
                                   epilog='Source code is available at https://github.com/ielizaga/gpcheckintegrity')
    parser.add_option('-v', '--verbose', action='store_true', help='Enables verbose logging')
    parser.add_option('-y', '--yes', action='store_true', help='Assumes yes and do not prompt to confirm')
    parser.add_option('-A', '--all', action='store_true', help='Perform the integrity check in all databases and tables')
    parser.add_option('-d', '--database', help='Target database')
    parser.add_option('-s', '--schema', help='Target schema')
    parser.add_option('-S', '--schema-list', dest='schema_list',
                      help='List of schemas to check, separated by commas. Example: s1,s2,s3')
    parser.add_option('-t', '--table', help='Target table')
    parser.add_option('-B', '--parallel', type=int, default=DEFAULT_NUM_THREADS,
                      help='Number of parallel workers (Default: 32)')
    (options, args_object) = parser.parse_args()

    USER = os.getenv('USER')
    if USER is None or USER is ' ':
        logger.error('USER environment variable must be set')
        parser.exit()

    if options.database is None and options.all is None:
        logger.error('A target database must be specified')
        parser.exit()

    if (options.database is not None and options.all is not None) or (options.schema is not None and options.schema-list is not None):
        logger.info('Some of the arguments are not compatible')
        parser.exit()

    if options.database is not None:
        logger.info("Checking integrity of database %s" % options.database)

    if options.all is not None:
        logger.info("Checking integrity of all databases")

    return options


def connect(user=None, password=None, host=None, port=None, database=None, utilityMode=False):
    conf = utilityMode and '-c gp_session_role=utility' or None
    if not user:
        user = os.environ.get('PGUSER')
    if not user:
        user = os.environ.get('USER')
    if not password:
        password = os.environ.get('PGPASSWORD')
    if not host:
        host = 'localhost'
    if not port:
        port = int(os.environ.get('PGPORT', 5432))
    if not database:
        database = os.environ.get('PGDATABASE', 'template1')
    try:
        logger.debug('connecting to %s:%s %s' % (host, port, database))
        db_conn = pg.connect(host=host, port=port, user=user,
                             passwd=password, dbname=database, opt=conf)
    except pg.InternalError, ex:
        logger.error('could not connect to %s: "%s"' %
                     (database, str(ex).strip()))
        return None

    logger.debug('connected with %s:%s %s' % (host, port, database))
    return db_conn


def get_gp_segment_configuration():
    cfg = {}
    db_conn = connect()

    if db_conn is None:
        logger.fatal('Unable to connect to master to get segment information. Aborting')
        raise TypeError('Connection to db_conn cannot be None')

    qry = '''
          SELECT content, preferred_role = 'p' as definedprimary,
                 dbid, role = 'p' as isprimary, hostname, address, port
            FROM gp_segment_configuration
            WHERE role = 'p' or content < 0
          '''
    curs = db_conn.query(qry)
    for row in curs.dictresult():
        if row['content'] == -1:
            continue  # skip master
        cfg[row['dbid']] = row
    db_conn.close()
    return cfg


def get_databases():
    """
    This function returns the list of databases present in the Greenplum cluster.
    :return: list of databases
    """
    db_conn = connect()

    if db_conn is None:
        logger.fatal('Unable to connect to master to obtain database names. Aborting')
        raise TypeError('Connection to db_conn cannot be None')

    database_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT datname
          FROM pg_database
          where datname not like 'template0'
          '''
    curs = db_conn.query(qry)
    for row in curs.dictresult():
        database_list.append(row)
    db_conn.close()
    return database_list


def get_tables(database=None):
    db_conn = connect(database=database)

    if db_conn is None:
        logger.fatal('Unable to connect to DB %s to get table names. Aborting' % database)
        raise TypeError('Connection to db_conn cannot be None')

    table_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT '%s' as database, n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname != 'pg_catalog') and (c.relhassubclass='f')
          ''' % pg.escape_string(database)
    curs = db_conn.query(qry)
    for row in curs.dictresult():
        table_list.append(row)
    db_conn.close()
    return table_list


def get_table(database, table_str):
    logger.debug("Parsing table string: %s" % table_str)
    table_cmdline_split = string.split(table_str, '.')

    if len(table_cmdline_split) == 1:
        schema_name = 'public'
        table_name = table_cmdline_split[0]
    elif len(table_cmdline_split) == 2:
        schema_name = table_cmdline_split[0]
        table_name = table_cmdline_split[1]
    else:
        logger.debug("Processing table string returned more than 2 fields")
        return None

    db_conn = connect(database=database)

    if db_conn is None:
        logger.fatal('Unable to connect to master to get segment information. Aborting')
        raise TypeError('Connection to db_conn cannot be None')

    qry = '''
        SELECT '%(database)s' as database, n.nspname as schema, c.relname as table
          FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
        WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname != 'pg_catalog')
        and (c.relname = '%(table_name)s') and (n.nspname = '%(schema_name)s')
    ''' % {"database": pg.escape_string(database), "table_name": pg.escape_string(table_name), "schema_name": pg.escape_string(schema_name)}
    
    curs = db_conn.query(qry)

    if curs.ntuples() == 0:
        db_conn.close()
        logger.warn("Table %s does not exists" % table_str)
        return None
    else:
        table_dict = curs.dictresult()
        db_conn.close()
        return table_dict


def get_tables_in_schema(database, schema):
    """
    This function returns a list of tables within :param schema:. If the schema is pg_catalog, it returns None and
    logs an error.
    :param database: name of the database
    :param schema: schema to fetch table names from
    :return: list of tables or None if :param schema: is 'pg_catalog'
    :raise ValueError: if schema is 'pg_catalog'
    """
    db_conn = connect(database=database)

    if db_conn is None:
        logger.fatal('Unable to connect to DB %s to get table names from schema %s. Aborting' % (database, schema))
        raise TypeError('Connection to db_conn cannot be None')

    table_list = []

    if schema == "pg_catalog":
        logger.error("This check is ineffective on catalog tables. Use gpcheckcat to check catalog integrity")
        raise ValueError("Schema is pg_catalog")

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT '%s' as database, n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname = '%s') and (c.relhassubclass='f')
          ''' % (pg.escape_string(database), pg.escape_string(schema))
    curs = db_conn.query(qry)

    for row in curs.dictresult():
        table_list.append(row)

    db_conn.close()

    return table_list


def database_schema_exists(database, schema):

    db_conn = connect(database=database)

    if db_conn is None:
        logger.fatal('Unable to connect to DB %s to check if schema %s exists. Aborting' % (database, schema))
        raise TypeError('Connection to db_conn cannot be None')

    qry = '''
    select True as exists from pg_namespace a where a.nspname = '%s'
    ''' % pg.escape_string(schema)

    logger.debug("Running query: %s" % qry)

    curs = db_conn.query(qry)

    return curs.ntuples() == 1

def _spawn_threads():

    tables = list()

    # Multiple schema scan
    if options.schema_list is not None:
        logger.debug("Parsing schema list: %s" % options.schema_list)
        schemas = string.split(options.schema_list, ',')
        for s in schemas:
            if database_schema_exists(options.database, s):
                logger.debug("Adding schema %s to checklist" % s)
                tables.extend(get_tables_in_schema(options.database, s))
            else:
                logger.warn("Schema %s not found" % s)

    # Single schema scan
    if options.schema is not None:
        tables.extend(get_tables_in_schema(options.database, options.schema))
        logger.info("Checking integrity of schema %s" % options.schema)
    
    # Single table scan
    if options.table is not None:
        table_dict = get_table(options.database, options.table)
        if table_dict is not None:
            tables.extend(table_dict)
    
    # Full database scan
    if options.all is None and options.table is None and options.schema is None and options.schema_list is None:
        tables.extend(get_tables(options.database))

    # All databases scan
    if options.all is not None:
        for db in get_databases():
            tables.extend(get_tables(db['datname']))

    # Did not pass user confirmation prompt
    if options.yes is not True \
            and prompt_user(tables) is False:
        logger.info('User cancelled the program, aborting...')
        sys.exit(0)

    dbids = get_gp_segment_configuration()  # get Greenplum segment information
    threads = []
    for dbid in dbids:
        if dbids[dbid]['isprimary'] == 't':
            th = CheckIntegrity(tables, dbids[dbid]['hostname'], dbids[dbid]['content'],
                                dbids[dbid]['port'])
            th.start()
            threads.append(th)

    for thread in threads:
        logger.debug('waiting on thread %s' % thread.getName())
        thread.join()

    return


def prompt_user(tables):
    logger.info('Precheck:')

    for table_obj in tables:
        logger.info('%(database)s - %(schema)s.%(table)s' % {'database': table_obj['database'], 'schema': table_obj['schema'],
                                                                 'table': table_obj['table']})
    while True:
        logger.info('\nDo you want to proceed? [Y/N]')
        line = sys.stdin.readline()

        if line[:-1] in ('Y', 'y', 'yes', 'Yes'):
            return True

        if line[:-1] in ('N', 'n', 'no', 'No'):
            return False


class CheckIntegrity(Thread):
    def __init__(self, tables, hostname, content, port):
        Thread.__init__(self)
        self.hostname = hostname
        self.port = port
        self.content = content
        self.tables = tables

    def run(self):
        pool_semaphore.acquire()
        counter = 1

        for table in self.tables:
            database = table['database']
            try:
                db_conn = connect(database=database, host=self.hostname, port=self.port, utilityMode=True)
                if db_conn is None:
                    logger.debug('Thread for seg%s failed to connect to the database %s. Exiting.' % (self.content,database))
                    pool_semaphore.release()
                    return
                logger.info("[seg%s] {%s} Checking table %s.%s [%s/%s]" % (
                    self.content, database, table['schema'], table['table'], counter, len(self.tables)))
                qry = '''
                      COPY "%s"."%s"
                      TO '/dev/null'
                      ''' % (table['schema'], table['table'])
                db_conn.query(qry)
                counter += 1
                db_conn.close()

            except DatabaseError, de:
                # TODO: better error summary report
                logger.error('[seg%s] {%s} Failed for table %s.%s' % (
                    self.content, database, table['schema'], table['table']))
                logger.debug('[seg%s] {%s} %s' % (self.content, database, str(de).strip()))

                # Append this table name to reported_table list
                table_lock.acquire()
                reported_tables.append(
                    "[seg%s] {%s} %s.%s in %s:%d" % (self.content, database, table['schema'], table['table'],
                                                     self.hostname, self.port))
                table_lock.release()

                # TODO: pygresql wraps all queries in the same cursor under the same transaction
                db_conn.close()
                db_conn = connect(database=database, host=self.hostname, port=self.port, utilityMode=True)
                if db_conn is None:
                    logger.error("Thread for seg%s found an unexpected error while trying to connect to the DB... "
                                 "Aborting" % self.content)
                    pool_semaphore.release()
                    return

            except Exception, unexpected_exception:
                logger.error("[seg%s] {%s} Failed with unexpected exception: " % (self.content, database))
                logger.error(unexpected_exception)
                break

        try:
            db_conn.close()
        except pg.InternalError, pgie:
            logger.debug(
                'Attempted to close an already closed connection. This probably means that a thread hit '
                'an unexpected error')
            logger.debug(pgie)

        pool_semaphore.release()


#############
if __name__ == '__main__':
    logger = get_default_logger()
    setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())

    options = parseargs()

    if options.verbose:
        enable_verbose_logging()

    try:
        reported_tables = []
        table_lock = Lock()

        logger.debug("Maximum number of threads is %d" % options.parallel)
        pool_semaphore = BoundedSemaphore(options.parallel)
        _spawn_threads()

        logger.info("ERROR REPORT SUMMARY %s" % datetime.now())
        logger.info("============================================")

        if len(reported_tables) == 0:
            logger.info("No tables reported inconsistency errors")
        else:
            for t in reported_tables:
                logger.info(t)

    # TODO: Better exception handling
    except Exception, e:
        logger.error('errors in job:')
        logger.error(e.__str__())
        logger.error('exiting early')

    logger.info("completed successfully")
